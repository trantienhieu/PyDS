{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://viknews.com/vi/wp-content/uploads/2019/10/machine-learning-l%C3%A0-g%C3%AC.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://machinelearningcoban.com/assets/introduce/aimldl.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://qph.fs.quoracdn.net/main-qimg-be1e3dcc84999c50f19369fdf81fbb01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning - Building models of data\n",
    "* Machine learning is where these computational and algorithmic skills of data science meet the statistical thinking of data science, and the result is **a collection of approaches to inference and data exploration** that are not about effective theory so much as effective computation.\n",
    "* The goals of our course are:\n",
    "    * To introduce the fundamental vocabulary and concepts of machine learning.\n",
    "    * To introduce the Scikit-Learn API and show some examples of its use.\n",
    "    * To take a deeper dive into the details of several of the most important machine learning approaches, and develop an intuition into how they work and when and where they are applicable.\n",
    "\n",
    "![](https://d2bwk5eec7cz2z.cloudfront.net/2018/10/cover.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://christophm.github.io/interpretable-ml-book/images/programing-ml.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What Machine Learning is\n",
    "![](https://images.squarespace-cdn.com/content/v1/53528f90e4b0768cad09d33b/1541142286409-D6HRDKTNDS9STFCYNKTD/ke17ZwdGBToddI8pDm48kGUZYzr-xc6m57ZkxOqp3wYUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYxCRW4BPu10St3TBAUQYVKcHhq3_ypEgcB5Pwp3Uy1Q2U2LlICdYQfW4R7e2r7hZ1u3WcC0TPI9pJVQCNLIpB4W/Capture9.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/max/791/1*iPgIcpnc-nzkigs6RaTZBw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What Machine Learning is NOT\n",
    "![](https://i.redd.it/ub141fjtpl521.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What Machine Learning is NOT\n",
    "![](https://i.pinimg.com/originals/fe/aa/1a/feaa1a16a315823b2d9ad24da7eccdaf.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What Machine Learning is NOT\n",
    "![](https://www.hardikp.com/assets/ml_is_hard_in_finance/ml_class_sketch.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What Machine Learning is NOT\n",
    "![](https://i.pinimg.com/originals/d7/39/13/d73913829decc601ea776e3b7774c4ea.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categories of Machine Learning\n",
    "* **Supervised learning** involves somehow modeling the relationship between measured features of data and some label associated with the data; once this model is determined, it can be used to apply labels to new, unknown data. This is further subdivided into:\n",
    "    * **Classification**: the labels are discrete categories, \n",
    "    * **Regression**: the labels are continuous quantities.\n",
    "* **Unsupervised learning** involves modeling the features of a dataset without reference to any label, and is often described as **\"letting the dataset speak for itself.\"**\n",
    "    * **Clustering** algorithms identify distinct groups of data.\n",
    "    * **Dimensionality reduction** algorithms search for more succinct representations of the data.\n",
    "![](https://www.mathworks.com/discovery/machine-learning/_jcr_content/mainParsys3/discoverysubsection_1965078453/mainParsys/image_2128876021_cop.adapt.full.low.svg/1586543170363.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.vas3k.ru/7w1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://blognoeliagonz.files.wordpress.com/2019/05/59756370_359693871329578_7078965161691709440_n.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualitative Examples of Machine Learning Applications\n",
    "## Classification: Predicting discrete labels\n",
    "* You are given a set of labeled points and want to use these to classify some unlabeled points.\n",
    "* Here we have two-dimensional data: that is, we have two features for each point, represented by the (x,y) positions of the points on the plane. \n",
    "* In addition, we have one of two class labels for each point, here represented by the colors of the points. \n",
    "* From these features and labels, we would like to create a model that will let us decide whether a new point should be labeled \"blue\" or \"red.\"\n",
    "![](https://jakevdp.github.io/PythonDataScienceHandbook/figures/05.01-classification-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualitative Examples of Machine Learning Applications\n",
    "## Classification: Predicting discrete labels\n",
    "* There are a number of possible models for such a classification task, but here we will use an extremely simple one. \n",
    "* We will make the assumption that the two groups can be separated by drawing a **straight line** through the plane between them, such that points on each side of the line fall in the same group. \n",
    "* Here the model is a quantitative version of the statement **\"a straight line separates the classes\"**, while the model parameters are the particular numbers describing the location and orientation of that line for our data. \n",
    "* The optimal values for these model parameters are learned from the data (this is the **\"learning\"** in machine learning), which is often called **training the model**.\n",
    "![](https://jakevdp.github.io/PythonDataScienceHandbook/figures/05.01-classification-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualitative Examples of Machine Learning Applications\n",
    "## Classification: Predicting discrete labels\n",
    "* Now that this model has been trained, it can be generalized to new, unlabeled data. In other words, we can take a new set of data, draw this model line through it, and assign labels to the new points based on this model. This stage is usually called **prediction**. See the following figure:\n",
    "![](https://jakevdp.github.io/PythonDataScienceHandbook/figures/05.01-classification-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualitative Examples of Machine Learning Applications\n",
    "## Classification: Predicting discrete labels\n",
    "* This is the basic idea of a **classification** task in machine learning, where **\"classification\"** indicates that the data has discrete class labels. \n",
    "* At first glance this may look fairly trivial: it would be relatively easy to simply look at this data and draw such a discriminatory line to accomplish this classification. A benefit of the machine learning approach, however, is that it can **generalize to much larger datasets in many more dimensions.**\n",
    "* For example, this is similar to the task of automated spam detection for email; in this case, we might use the following features and labels:\n",
    "    * feature 1, feature 2, etc. → normalized counts of important words or phrases (\"Viagra\", \"Nigerian prince\", etc.)\n",
    "    * label → \"spam\" or \"not spam\"\n",
    "\n",
    "![](https://i.vas3k.ru/7w2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualitative Examples of Machine Learning Applications\n",
    "## Regression: Predicting continuous labels\n",
    "* In contrast with the discrete labels of a classification algorithm, we will next look at a simple regression task in which the labels are continuous quantities.\n",
    "* Consider the data shown in the following figure, which consists of a set of points each with a continuous label:\n",
    "![](https://jakevdp.github.io/PythonDataScienceHandbook/figures/05.01-regression-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualitative Examples of Machine Learning Applications\n",
    "## Regression: Predicting continuous labels\n",
    "* As with the classification example, we have two-dimensional data: that is, there are two features describing each data point. The color of each point represents the continuous label for that point.\n",
    "* There are a number of possible regression models we might use for this type of data, but here we will use a simple linear regression to predict the points. \n",
    "* This simple linear regression model assumes that if we treat the label as a third spatial dimension, we can fit a plane to the data. This is a higher-level generalization of the well-known problem of fitting a line to data with two coordinates.\n",
    "* We can visualize this setup as shown in the following figure:\n",
    "![](https://jakevdp.github.io/PythonDataScienceHandbook/figures/05.01-regression-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualitative Examples of Machine Learning Applications\n",
    "## Regression: Predicting continuous labels\n",
    "* Notice that the feature 1-feature 2 plane here is the same as in the two-dimensional plot from before.\n",
    "* In this case, however, we have represented the labels by both color and three-dimensional axis position. \n",
    "* From this view, it seems reasonable that fitting a plane through this three-dimensional data would allow us to predict the expected label for any set of input parameters. \n",
    "* Returning to the two-dimensional projection, when we fit such a plane we get the result shown in the following figure:\n",
    "![](https://jakevdp.github.io/PythonDataScienceHandbook/figures/05.01-regression-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualitative Examples of Machine Learning Applications\n",
    "## Regression: Predicting continuous labels\n",
    "* This plane of fit gives us what we need to predict labels for new points. \n",
    "* As with the classification example, this may seem rather trivial in a low number of dimensions. But the power of these methods is that they can be straightforwardly applied and evaluated in the case of data with many, many features.\n",
    "* Visually, we find the results shown in the following figure:\n",
    "![](https://jakevdp.github.io/PythonDataScienceHandbook/figures/05.01-regression-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualitative Examples of Machine Learning Applications\n",
    "## Clustering: Inferring labels on unlabeled data\n",
    "* The classification and regression illustrations we just looked at are examples of **supervised learning** algorithms, in which we are trying to build a model that will predict labels for new data. \n",
    "* **Unsupervised learning** involves models that describe data without reference to any known labels.\n",
    "* One common case of unsupervised learning is **\"clustering,\"** in which data is automatically assigned to some number of discrete groups. For example, we might have some two-dimensional data like that shown in the following figure:\n",
    "![](https://jakevdp.github.io/PythonDataScienceHandbook/figures/05.01-clustering-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualitative Examples of Machine Learning Applications\n",
    "## Clustering: Inferring labels on unlabeled data\n",
    "* By eye, it is clear that each of these points is part of a distinct group. Given this input, a clustering model will use the intrinsic structure of the data to determine which points are related. Using the very fast and intuitive **k-means algorithm**, we find the clusters shown in the following figure.\n",
    "* **k-means** fits a model consisting of k cluster centers; the optimal centers are assumed to be those that minimize the distance of each point from its assigned center. \n",
    "* Again, this might seem like a trivial exercise in two dimensions, but as our data becomes larger and more complex, such clustering algorithms can be employed to extract useful information from the dataset.\n",
    "![](https://jakevdp.github.io/PythonDataScienceHandbook/figures/05.01-clustering-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualitative Examples of Machine Learning Applications\n",
    "## Dimensionality reduction: Inferring structure of unlabeled data\n",
    "* **Dimensionality reduction** is another example of an unsupervised algorithm, in which labels or other information are inferred from the structure of the dataset itself. \n",
    "* **Dimensionality reduction** is a bit more abstract than the examples we looked at before, but generally it seeks to pull out some low-dimensional representation of data that in some way preserves relevant qualities of the full dataset. \n",
    "* Different **dimensionality reduction** routines measure these relevant qualities in different ways.\n",
    "* As an example of this, consider the data shown in the following figure:\n",
    "![](https://jakevdp.github.io/PythonDataScienceHandbook/figures/05.01-dimesionality-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualitative Examples of Machine Learning Applications\n",
    "## Dimensionality reduction: Inferring structure of unlabeled data\n",
    "* Visually, it is clear that there is some structure in this data: it is drawn from a one-dimensional line that is arranged in a spiral within this two-dimensional space. \n",
    "* In a sense, you could say that this data is \"intrinsically\" only one dimensional, though this one-dimensional data is embedded in higher-dimensional space. \n",
    "* A suitable dimensionality reduction model in this case would be sensitive to this nonlinear embedded structure, and be able to pull out this lower-dimensionality representation.\n",
    "* The following figure shows a visualization of the results of the Isomap algorithm, a manifold learning algorithm that does exactly this:\n",
    "![](https://jakevdp.github.io/PythonDataScienceHandbook/figures/05.01-dimesionality-2.png)\n",
    "* Notice that the colors (which represent the extracted one-dimensional latent variable) change uniformly along the spiral, which indicates that the algorithm did in fact detect the structure we saw by eye. As with the previous examples, the power of dimensionality reduction algorithms becomes clearer in higher-dimensional cases. For example, we might wish to visualize important relationships within a dataset that has 100 or 1,000 features. Visualizing 1,000-dimensional data is a challenge, and one way we can make this more manageable is to use a dimensionality reduction technique to reduce the data to two or three dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "* Here we have seen a few simple examples of some of the basic types of machine learning approaches. Needless to say, there are a number of important practical details that we have glossed over, but I hope this section was enough to give you a basic idea of what types of problems machine learning approaches can solve.\n",
    "* In short, we saw the following:\n",
    "    * Supervised learning: Models that can predict labels based on labeled training data\n",
    "        * Classification: Models that predict labels as two or more discrete categories\n",
    "        * Regression: Models that predict continuous labels\n",
    "    * Unsupervised learning: Models that identify structure in unlabeled data\n",
    "        * Clustering: Models that detect and identify distinct groups in the data\n",
    "        * Dimensionality reduction: Models that detect and identify lower-dimensional structure in higher-dimensional data  \n",
    "![](https://image.slidesharecdn.com/mlintro-120730222641-phpapp01/95/introduction-to-machine-learning-11-728.jpg?cb=1479906039)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Multiple Linear Regression from Scratch in Numpy\n",
    "## How does the model actually work behind the scenes?\n",
    "![](https://slideplayer.com/slide/15760442/88/images/7/Basic+API+estimator.fit%28X%2C+%5By%5D%29+estimator.predict+estimator.transform.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Multiple Linear Regression from Scratch in Numpy\n",
    "## How does the model actually work behind the scenes?\n",
    "### Matrix multiplication\n",
    "![](https://miro.medium.com/max/3688/1*D_1tbv_wNFJ-rrremAGX4Q.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Multiple Linear Regression from Scratch in Numpy\n",
    "## How does the model actually work behind the scenes?\n",
    "### Matrix inverse\n",
    "![](https://www.onlinemathlearning.com/image-files/inverse-matrix.png)\n",
    "![](https://calclab.math.tamu.edu/~ssanm/invform.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Multiple Linear Regression from Scratch in Numpy\n",
    "## How does the model actually work behind the scenes?\n",
    "### Matrix transpose\n",
    "![](https://miro.medium.com/max/3200/1*jxk8ReH4QtGsqcVC6Dr1Sg.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Multiple Linear Regression from Scratch in Numpy\n",
    "## How does the model actually work behind the scenes?\n",
    "![](https://i.imgur.com/DT4H1Yk.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Multiple Linear Regression from Scratch in Numpy\n",
    "## How does the model actually work behind the scenes?\n",
    "![](https://www.jmp.com/en_us/statistics-knowledge-portal/what-is-multiple-regression/fitting-multiple-regression-model/_jcr_content/par/styledcontainer_2069/par/lightbox_4130/lightboxImage.img.png/1548702778023.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Multiple Linear Regression from Scratch in Numpy\n",
    "## How does the model actually work behind the scenes?\n",
    "![](https://slideplayer.com/slide/5131226/16/images/37/Example+from+Multiple+Linear+Regression+I.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Multiple Linear Regression from Scratch in Numpy\n",
    "## How does the model actually work behind the scenes?\n",
    "![](https://slideplayer.com/slide/5131226/16/images/38/Example+from+Multiple+Linear+Regression+II.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Multiple Linear Regression from Scratch in Numpy\n",
    "## How does the model actually work behind the scenes?\n",
    "[Click to download data](https://raw.githubusercontent.com/selva86/datasets/master/BostonHousing.csv)\n",
    "![](https://miro.medium.com/max/253/1*D5H7igmxKerAr0Dr9Dq-Tg.png)\n",
    "![](https://miro.medium.com/max/1400/1*wvDDeJKkHcFZnXkZjqvyzw.png)\n",
    "![](https://miro.medium.com/max/1318/1*EmPT-2jzZ62vXLdO_feMUw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Multiple Linear Regression from Scratch in Numpy\n",
    "## How does the model actually work behind the scenes?\n",
    "![](https://miro.medium.com/max/572/1*jwui_z0SrvpJao-XSnYB6A.png)\n",
    "![](https://miro.medium.com/max/230/1*A350MsVMeYYnAYC35BkNnQ.png)\n",
    "![](https://miro.medium.com/max/1096/1*CmEpjhQJQWZAzjPxkd04HQ.png)\n",
    "![](https://miro.medium.com/max/716/1*ztR6cTFPY4ITHxeFFfCS1A.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework: Please reproduce the above example\n",
    "![](https://previews.123rf.com/images/enterline/enterline1707/enterline170700327/82358188-the-word-homework-concept-and-theme-painted-in-black-ink-on-a-watercolor-wash-background-.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
